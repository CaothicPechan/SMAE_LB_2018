{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABC News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/benjamin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publish_date,headline_text\r\n",
      "20030219,aba decides against community broadcasting licence\r\n",
      "20030219,act fire witnesses must be aware of defamation\r\n",
      "20030219,a g calls for infrastructure protection summit\r\n",
      "20030219,air nz staff in aust strike for pay rise\r\n",
      "20030219,air nz strike to affect australian travellers\r\n",
      "20030219,ambitious olsson wins triple jump\r\n",
      "20030219,antic delighted with record breaking barca\r\n",
      "20030219,aussie qualifier stosur wastes four memphis match\r\n",
      "20030219,aust addresses un security council over iraq\r\n"
     ]
    }
   ],
   "source": [
    "!head abcnews-date-text.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('abcnews-date-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1103665"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = len(df)\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning - Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence, stopwords):\n",
    "    return [x for x in sentence if x not in stopwords]\n",
    "\n",
    "def stem_words(tokens, stemmer):\n",
    "    return [stemmer.stem(x) for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String to lower\n",
    "df[\"USE_OF_PROCEEDS\"] = df[\"headline_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize - this adds the square brackets\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"USE_OF_PROCEEDS\"] = df[\"USE_OF_PROCEEDS\"].apply(tokenizer.tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "df[\"USE_OF_PROCEEDS\"] = df[\"USE_OF_PROCEEDS\"].apply(remove_stopwords, args=[stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem words --- Identify roots to supress multiplicity -- e.g. inversion, invertir..\n",
    "# common NLP technique to reduce topically similar words to their root.\n",
    "stemmer = SnowballStemmer('english')\n",
    "df[\"USE_OF_PROCEEDS\"] = df[\"USE_OF_PROCEEDS\"].apply(stem_words, args=[stemmer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>USE_OF_PROCEEDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>[aba, decid, communiti, broadcast, licenc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>[act, fire, wit, must, awar, defam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>[g, call, infrastructur, protect, summit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>[air, nz, staff, aust, strike, pay, rise]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>[air, nz, strike, affect, australian, travel]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text  \\\n",
       "0      20030219  aba decides against community broadcasting lic...   \n",
       "1      20030219     act fire witnesses must be aware of defamation   \n",
       "2      20030219     a g calls for infrastructure protection summit   \n",
       "3      20030219           air nz staff in aust strike for pay rise   \n",
       "4      20030219      air nz strike to affect australian travellers   \n",
       "\n",
       "                                 USE_OF_PROCEEDS  \n",
       "0     [aba, decid, communiti, broadcast, licenc]  \n",
       "1            [act, fire, wit, must, awar, defam]  \n",
       "2      [g, call, infrastructur, protect, summit]  \n",
       "3      [air, nz, staff, aust, strike, pay, rise]  \n",
       "4  [air, nz, strike, affect, australian, travel]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We just need this colum\n",
    "training_docset = df['USE_OF_PROCEEDS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aba'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flatten in one single string all words i.e. training_docset panda series has to\n",
    "#conform to the datastructure of e.g. the fetch_20newsgroups dataset in sklearn\n",
    "data_samples = [item for sublist in training_docset.tolist() for item in sublist]\n",
    "type(data_samples)\n",
    "data_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA/HMF Model and document-term matrix Construction (Sklearn only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are goinbg to use sklearn only, its lighter. (e.g. AWS lambda applications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1103665"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 1000 \n",
    "n_samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting tf features for LDA. Remove words ocurring in only one doc or in 95% of the total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When building the vocabulary ignore terms that have a document frequency strictly \n",
    "#higher than the given threshold (corpus-specific stop words)\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_features=n_features,max_df=0.95, min_df=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware: the standard definition of the tf's and the sklearn implementation differe, see https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5864176, 1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf= tf_vectorizer.fit_transform(data_samples)\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=n_features,max_df=0.95, min_df=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5864176, 1000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf= tfidf_vectorizer.fit_transform(data_samples)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA: Attention: iterations or pases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=1103665 and n_features=1000...\n",
      "done in 793.959s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: australian trump melbourn world sa adelaid\n",
      "Topic #1: us govern win day attack kill\n",
      "Topic #2: australia new polic wa court call\n",
      "Topic #3: say year nsw hous perth canberra\n",
      "Topic #4: man sydney charg elect queensland death\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_components = 5  # Number of Topics\n",
    "n_top_words = 6\n",
    "model=1\n",
    "\n",
    "if model==1:\n",
    "   print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"  % (n_samples, n_features))\n",
    "   lda = LatentDirichletAllocation(n_components=n_components, max_iter=2,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "   t0 = time()\n",
    "   lda.fit(tf)\n",
    "   print(\"done in %0.3fs.\" % (time() - t0))\n",
    "   print(\"\\nTopics in LDA model:\")\n",
    "   tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "   print_top_words(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "elif model==2:\n",
    "\n",
    "  # Fit the NMF model\n",
    "  print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "  t0 = time()\n",
    "  nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "  print(\"donee in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "  print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "  tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "  print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity!\n",
    "\n",
    "New South Wales? Sa, South Africa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Model performance diagnostics* using the  perplexity and the score (log-likelihood). \n",
    "A model with higher score (log-likelihood) and lower perplexity (exp(-1. * log-likelihood per word)) \n",
    "is considered to be good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for the model: Perplexity=827.634822 and score=-24181108.178084...\n"
     ]
    }
   ],
   "source": [
    "print(\"Metrics for the model: Perplexity=%f and score=%f...\"\n",
    "      % (lda.perplexity(tf), lda.score(tf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=1103665 and n_features=1000...\n",
      "donee in 113.926s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: polic zone festiv first firm firefight\n",
      "Topic #1: man zone fee firm firefight fire\n",
      "Topic #2: new zone festiv first firm firefight\n",
      "Topic #3: plan zone first firm firefight fire\n",
      "Topic #4: call festiv fish first firm firefight\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_components = 5  # Number of Topics\n",
    "n_top_words = 6\n",
    "model=2\n",
    "\n",
    "if model==1:\n",
    "   print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"  % (n_samples, n_features))\n",
    "   lda = LatentDirichletAllocation(n_components=n_components, max_iter=2,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "   t0 = time()\n",
    "   lda.fit(tf)\n",
    "   print(\"done in %0.3fs.\" % (time() - t0))\n",
    "   print(\"\\nTopics in LDA model:\")\n",
    "   tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "   print_top_words(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "elif model==2:\n",
    "\n",
    "  # Fit the NMF model\n",
    "  print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "  t0 = time()\n",
    "  nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "  print(\"donee in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "  print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "  tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "  print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = 'lda_news.sav'\n",
    "pickle.dump(lda, open(filename1, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename3 = 'tf_news.sav'\n",
    "pickle.dump(tf, open(filename3, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename4 = 'tfve_news.sav'\n",
    "pickle.dump(tf_vectorizer, open(filename4, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
