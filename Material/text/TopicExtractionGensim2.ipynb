{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Example LDA Topic Extraction - Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- English fo simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import/Create documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning your documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenizing: converting a document to its atomic elements.\n",
    "\n",
    "    \n",
    "- Stopping: removing meaningless words.\n",
    "\n",
    "    \n",
    "- Stemming: merging words that are equivalent in meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "- Tokenization segments a document into its atomic elements. \n",
    "\n",
    "\n",
    "- In this case, we are interested in tokenizing to words. \n",
    "\n",
    "\n",
    "- Tokenization can be performed many ways–we are using NLTK’s tokenize.regexp module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ``RegexpTokenizer`` splits a string into substrings using a regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = doc_a.lower()\n",
    "tokens = tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'is', 'good', 'to', 'eat', 'my', 'brother', 'likes', 'to', 'eat', 'good', 'brocolli', 'but', 'not', 'my', 'mother']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "- Conjunctions (“for”, “or”) or  “the” are meaningless to a topic model. \n",
    "\n",
    "\n",
    "- These terms are called stop words and need to be removed from our token list.\n",
    "\n",
    "\n",
    "- Example inconvenience:  topic modeling a collection of music reviews, then terms like \n",
    "  “The Who” will have trouble \n",
    "\n",
    "\n",
    "- Freedom to construct your own stop word list (like we do in Spanish)\n",
    "\n",
    "\n",
    "- stop_words package from Pypi, a relatively conservative list. \n",
    "\n",
    "\n",
    "- We can call get_stop_words() to create a list of stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " \"here's\",\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " \"how's\",\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"let's\",\n",
       " 'me',\n",
       " 'more',\n",
       " 'most',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'same',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that's\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " \"there's\",\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'very',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'when',\n",
       " \"when's\",\n",
       " 'where',\n",
       " \"where's\",\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " \"who's\",\n",
       " 'whom',\n",
       " 'why',\n",
       " \"why's\",\n",
       " 'with',\n",
       " \"won't\",\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "en_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "- NLP technique to reduce topically similar words to their root. \n",
    "\n",
    "\n",
    "- For example, “stemming,” “stemmer,” “stemmed,” all have similar meanings; \n",
    "\n",
    "\n",
    "- stemming reduces those terms to “stem. (tallo)” \n",
    "\n",
    "\n",
    "- This is important for topic modeling, which would otherwise view those terms as separate entities and \n",
    "  reduce their importance in the model.\n",
    "\n",
    "    \n",
    "- One option:  The Porter stemming algorithm is the most widely used method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list for tokenized documents in loop\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a document-term matrix using Genism\n",
    "\n",
    "- Gensim\n",
    "- author(s): Radim Řehůřek\n",
    "- Initial release: 2009\n",
    "- Gensim is a robust open-source topic modeling toolkit implemented in Python.\n",
    "- It uses NumPy, SciPy and optionally Cython for performance. \n",
    "- Gensim is specifically designed to handle large text collections\n",
    "- Streaming ->  differentiates it from most other scientific software packages that only target batch and in-memory processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To generate an LDA model, we need to understand how **frequently** each term occurs within each document. \n",
    "- We need to construct a document-term matrix with  gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Dictionary() function traverses texts, \n",
    "- assigning a unique integer id to each unique token \n",
    "- Also collecting word counts and relevant statistics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see each token’s unique integer id, try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blood': 13, 'expert': 15, 'like': 4, 'suggest': 20, 'around': 6, 'drive': 8, 'well': 29, 'good': 3, 'often': 25, 'pressur': 19, 'brocolli': 0, 'may': 18, 'lot': 9, 'school': 27, 'better': 22, 'increas': 17, 'caus': 14, 'brother': 1, 'mother': 5, 'seem': 28, 'basebal': 7, 'never': 24, 'feel': 23, 'health': 16, 'say': 31, 'eat': 2, 'practic': 10, 'spend': 11, 'time': 12, 'perform': 26, 'tension': 21, 'profession': 30}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, our dictionary must be converted into a bag-of-words:\n",
    "    \n",
    "- The bag-of-words model is a simplifying representation used in natural language processing \n",
    "\n",
    "\n",
    "- In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words\n",
    "\n",
    "\n",
    "- **Disregarding grammar and even word order but keeping multiplicity**\n",
    "\n",
    "\n",
    "- The bag-of-words model has also been used for computer vision.[1]\n",
    "\n",
    "\n",
    "- In document classification (frequency of) occurrence of each word is used as a feature.\n",
    "\n",
    "-- First reference in literature: Zellig Harris's 1954."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The doc2bow() (Document to bag of words) function converts dictionary into a bag-of-words. \n",
    "\n",
    "\n",
    "- The result, corpus, is a list of vectors equal to the number of documents. \n",
    "\n",
    "\n",
    "- In each document vector is a series of tuples. \n",
    "\n",
    "\n",
    "- As an example, print(corpus[0]) results in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- This list of tuples represents our first document, doc_a. \n",
    "\n",
    "\n",
    "- The tuples are (term ID, term frequency) pairs, \n",
    "\n",
    "\n",
    "- If  print(dictionary.token2id) says brocolli’s id is 0, then the first tuple indicates that brocolli\n",
    " appeared twice in doc_a. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **corpus** is a document-term matrix and now we’re ready to generate an LDA model.\n",
    "\n",
    "\n",
    "- The LdaModel class is described in detail in the gensim documentation. \n",
    "\n",
    "\n",
    "- Parameters:\n",
    "  - **num_topics**: User determine how many topics should be generated. Our document set is small, so we’re only asking for three topics.\n",
    "    \n",
    "  - **id2word**: required. The LdaModel class requires our previous **dictionary to map ids to strings.**\n",
    "\n",
    "  - **passes:** optional. The number of **laps** the model will take through corpus. \n",
    "\n",
    "        \n",
    "        \n",
    "- The greater the number of passes, the more **accurate** the model will be. \n",
    "\n",
    "\n",
    "\n",
    "- A lot of passes can be **slow** on a very large corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### we specify the number of topics (clusters). We arrive to the non-elegant result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.074*\"drive\" + 0.074*\"brother\" + 0.074*\"mother\" + 0.074*\"spend\"'),\n",
       " (1, '0.135*\"health\" + 0.052*\"pressur\" + 0.052*\"suggest\" + 0.052*\"expert\"'),\n",
       " (2, '0.031*\"drive\" + 0.031*\"mother\" + 0.031*\"brother\" + 0.031*\"brocolli\"'),\n",
       " (3, '0.078*\"good\" + 0.078*\"brocolli\" + 0.078*\"eat\" + 0.078*\"mother\"')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=4, num_words=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "of course very small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
